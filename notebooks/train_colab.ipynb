{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIMBA Training on Google Colab/Kaggle\n",
    "\n",
    "This notebook trains DIMBA (Diffusion-based Mamba) on GPU with:\n",
    "- mamba2-ssm optimized library\n",
    "- BPE tokenizer\n",
    "- HuggingFace datasets\n",
    "\n",
    "Works on Google Colab and Kaggle with T4/A100 GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch pytorch-lightning transformers datasets\n",
    "!pip install -q tokenizers  # For BPE tokenizer\n",
    "!pip install -q causal-conv1d  # Required for mamba-ssm\n",
    "!pip install -q mamba-ssm  # Optimized Mamba library\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone or mount repo\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# For Colab: mount Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    repo_path = '/content/drive/MyDrive/dimba-lib-exp'  # Adjust path as needed\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    # Kaggle or local\n",
    "    repo_path = '/kaggle/input/dimba-lib-exp' if os.path.exists('/kaggle') else '.'\n",
    "    IS_COLAB = False\n",
    "\n",
    "os.chdir(repo_path)\nprint(f\"Working directory: {os.getcwd()}\")\nprint(f\"Is Colab: {IS_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to path and import\n",
    "import sys\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from dimba import DIMBA\n",
    "from dimba.tokenizers import BPETokenizer, SimpleCharacterTokenizer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Config loaded:\")\nprint(f\"  Model: d_model={config['model']['d_model']}, layers={config['model']['num_denoiser_layers']}\")\nprint(f\"  Data: {config['data']['type']}\")\nprint(f\"  Tokenizer: {config['tokenizer']['type']}, vocab_size={config['tokenizer']['vocab_size']}\")\nprint(f\"  Training: lr={config['training']['learning_rate']}, epochs={config['training']['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device\n",
    "device_config = config.get('device', {})\n",
    "use_gpu = device_config.get('use_gpu', True) and torch.cuda.is_available()\n",
    "device = 'cuda' if use_gpu else 'cpu'\n",
    "\n",
    "if use_gpu:\n",
    "    torch.cuda.set_per_process_memory_fraction(0.8)  # Use 80% of GPU memory\n",
    "    if device_config.get('benchmark', True):\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"Device: {device}\")\nif use_gpu:\n",
    "    print(f\"  Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_type = config['tokenizer']['type']\nvocab_size = config['tokenizer']['vocab_size']\n\n# Create appropriate tokenizer\nif tokenizer_type == 'bpe':\n    print(f\"Creating BPE tokenizer (vocab_size={vocab_size})...\")\n    try:\n        # Try to load existing tokenizer\n        tokenizer = BPETokenizer(vocab_size=vocab_size)\n        print(\"  Note: Using untrained tokenizer. For real training, train on your dataset first.\")\n    except Exception as e:\n        print(f\"  Creating new BPE tokenizer: {e}\")\n        tokenizer = BPETokenizer(vocab_size=vocab_size)\nelse:\n",
    "    print(f\"Creating SimpleCharacterTokenizer (vocab_size={vocab_size})...\")\n",
    "    tokenizer = SimpleCharacterTokenizer(vocab_size=vocab_size)\n",
    "\nprint(f\"✓ Tokenizer created\")\nprint(f\"  vocab_size: {tokenizer.vocab_size}\")\nprint(f\"  pad_token_id: {tokenizer.pad_token_id}\")\nprint(f\"  unk_token_id: {tokenizer.unk_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DIMBA model\nmodel_config = config['model']\nmodel = DIMBA(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=model_config['d_model'],\n",
    "    d_prompt=model_config['d_prompt'],\n",
    "    num_diffusion_steps=model_config['num_diffusion_steps'],\n",
    "    num_denoiser_layers=model_config['num_denoiser_layers'],\n",
    "    d_state=model_config['d_state'],\n",
    "    d_conv=model_config['d_conv'],\n",
    "    expand=model_config['expand'],\n",
    "    conditioning_type=model_config['conditioning_type'],\n",
    "    dropout=model_config['dropout'],\n",
    "    use_weight_tying=model_config['use_weight_tying'],\n",
    "    use_simple_mamba=model_config.get('use_simple_mamba', False),  # Use mamba-ssm on GPU\n",
    ")\n",
    "\nmodel = model.to(device)\n",
    "\n",
    "# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\nprint(f\"✓ Model created\")\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")\nif use_gpu:\n",
    "    print(f\"  GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data"
   ]
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset # Directly import load_dataset\nimport importlib\nimport dimba.data.dataset # For collate_fn if needed\nfrom dimba.data import DummyDataset, collate_fn # Keep collate_fn, potentially DummyDataset\n\ndata_config = config['data']\ndata_type = data_config['type']\nbatch_size = data_config['batch_size']\nmax_length = data_config['max_length']\n\nprint(f\"Creating {data_type} dataset...\")\n\nif data_type == 'dummy':\n    train_dataset = DummyDataset(\n        size=data_config.get('num_examples', 1000),\n        vocab_size=vocab_size,\n        seq_length=max_length,\n    )\n    val_dataset = DummyDataset(\n        size=data_config.get('num_examples', 1000) // 10,\n        vocab_size=vocab_size,\n        seq_length=max_length,\n    )\nelif data_type == 'huggingface':\n    dataset_name_from_config = data_config.get('dataset_name', 'wikitext')\n    dataset_config_from_config = data_config.get('dataset_config', 'wikitext-2-raw-v1') # Default to wikitext-2-raw-v1\n\n    print(f\"Loading Hugging Face dataset: {dataset_name_from_config} with config: {dataset_config_from_config}\")\n\n    # Load raw datasets directly using datasets.load_dataset\n    raw_train_dataset = load_dataset(dataset_name_from_config, name=dataset_config_from_config, split='train', streaming=False)\n    raw_val_dataset = load_dataset(dataset_name_from_config, name=dataset_config_from_config, split='validation', streaming=False)\n\n    # Limit number of examples if specified\n    num_train_examples = data_config.get('num_examples', 10000)\n    if num_train_examples:\n        raw_train_dataset = raw_train_dataset.select(range(num_train_examples))\n\n    num_val_examples = 1000\n    if num_val_examples:\n        raw_val_dataset = raw_val_dataset.select(range(num_val_examples))\n\n    # Define a simple wrapper for tokenization and compatibility with DataLoader\n    class CustomHuggingFaceDataset(Dataset):\n        def __init__(self, hf_dataset, tokenizer, max_length, text_column='text'):\n            self.hf_dataset = hf_dataset\n            self.tokenizer = tokenizer\n            self.max_length = max_length\n            self.text_column = text_column\n\n        def __len__(self):\n            return len(self.hf_dataset)\n\n        def __getitem__(self, idx):\n            example = self.hf_dataset[idx]\n            text = example.get(self.text_column)\n            \n            # Handle potential list of lists for wikitext-2-raw-v1 and None values\n            if isinstance(text, list):\n                text = ' '.join([str(item) for item in text if item is not None])\n            elif text is None:\n                text = \"\" # Ensure text is a string for tokenizer\n\n            tokenized = self.tokenizer.encode(\n                text,\n                padding=False, # Let collate_fn handle padding\n                truncation=True,\n                max_length=self.max_length,\n                add_special_tokens=True\n            )\n            # Handle both list and BatchEncoding returns\n            if isinstance(tokenized, list):\n                input_ids = tokenized\n            else:\n                input_ids = tokenized['input_ids']\n            \n            return {'input_ids': input_ids}\n\n    # Instantiate custom datasets\n    train_dataset = CustomHuggingFaceDataset(raw_train_dataset, tokenizer, max_length)\n    val_dataset = CustomHuggingFaceDataset(raw_val_dataset, tokenizer, max_length)\n\n# DataLoader creation remains the same, as long as train_dataset and val_dataset are Dataset objects\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=0,\n    pin_memory=use_gpu,\n    collate_fn=collate_fn,\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    num_workers=0,\n    pin_memory=use_gpu,\n    collate_fn=collate_fn,\n)\n\nprint(f\"✓ Datasets created\")\nprint(f\"  Train samples: {len(train_dataset)}\")\nprint(f\"  Val samples: {len(val_dataset)}\")\nprint(f\"  Batch size: {batch_size}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\ntrain_config = config['training']\n",
    "\n# Create optimizer\noptimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=train_config['learning_rate'],\n",
    "    weight_decay=train_config['weight_decay'],\n",
    ")\n",
    "\n# Create scheduler\nnum_epochs = train_config['num_epochs']\ntotal_steps = num_epochs * len(train_loader)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_steps)\n",
    "\nprint(f\"✓ Optimizer & scheduler created\")\nprint(f\"  Learning rate: {train_config['learning_rate']}\")\nprint(f\"  Total training steps: {total_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nfrom tqdm.notebook import tqdm\n",
    "\ndef train_epoch(model, train_loader, optimizer, scheduler, device, epoch, log_interval=100):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        \n",
    "        # Sample random timesteps\n",
    "        t = torch.randint(0, model.num_diffusion_steps, (input_ids.shape[0],), device=device)\n",
    "        \n",
    "        # Forward pass\n",
    "        x_pred, noise = model(input_ids, t)\n",
    "        \n",
    "        # Get embeddings\n",
    "        x_0 = model.token_embed(input_ids)\n",
    "        \n",
    "        # MSE loss\n",
    "        loss = torch.nn.functional.mse_loss(x_pred, x_0)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if train_config.get('gradient_clip'):\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), train_config['gradient_clip'])\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            avg_loss = np.mean(losses[-log_interval:])\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\ndef validate(model, val_loader, device):\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            \n",
    "            t = torch.randint(0, model.num_diffusion_steps, (input_ids.shape[0],), device=device)\n",
    "            x_pred, noise = model(input_ids, t)\n",
    "            x_0 = model.token_embed(input_ids)\n",
    "            loss = torch.nn.functional.mse_loss(x_pred, x_0)\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\nprint(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\nnum_epochs = train_config['num_epochs']\nbest_val_loss = float('inf')\n",
    "\nfor epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(\n",
    "        model, train_loader, optimizer, scheduler, device, epoch,\n",
    "        log_interval=train_config.get('log_interval', 100)\n",
    "    )\n",
    "    \n",
    "    val_loss = validate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Train loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint_path = f\"./checkpoints/dimba-epoch={epoch:02d}-val_loss={val_loss:.4f}.ckpt\"\n",
    "        os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"  ✓ Saved checkpoint: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\nfinal_checkpoint = f\"./checkpoints/dimba-final-val_loss={best_val_loss:.4f}.ckpt\"\nos.makedirs(os.path.dirname(final_checkpoint), exist_ok=True)\ntorch.save(model.state_dict(), final_checkpoint)\nprint(f\"✓ Saved final model: {final_checkpoint}\")\n",
    "\n# Save tokenizer\ntokenizer_path = \"./checkpoints/tokenizer.json\"\ntokenizer.save(tokenizer_path)\nprint(f\"✓ Saved tokenizer: {tokenizer_path}\")\n",
    "\nif IS_COLAB:\n",
    "    print(\"\\nTo download checkpoints:\")\n",
    "    print(f\"  1. Go to Google Drive/dimba-lib-exp/checkpoints/\")\n",
    "    print(f\"  2. Download .ckpt files\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}