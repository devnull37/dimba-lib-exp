# DIMBA Model Configuration

# Model architecture parameters
model:
  d_model: 256              # Hidden dimension (reduced for CPU testing)
  d_prompt: 128             # Prompt conditioning dimension
  num_diffusion_steps: 1000 # Number of diffusion steps T
  num_denoiser_layers: 6    # Number of Mamba-2 blocks (reduced for testing)
  d_state: 16               # SSM state size
  d_conv: 4                 # Convolution kernel size
  expand: 2                 # Expansion factor for Mamba
  conditioning_type: "film" # 'film' or 'additive'
  dropout: 0.1
  use_weight_tying: false   # Tie embedding and output weights
  use_simple_mamba: false   # Set to true to force SimpleMamba2 (CPU), false for mamba-ssm (GPU)
  latent_diffusion: false   # Enable diffusion in a latent subspace
  d_latent: null            # Latent dimension (defaults to d_model // 2)
  latent_projector_depth: 2 # MLP depth for latent encode/decode
  latent_loss_weight: 1.0   # Weight for latent-space MSE loss
  recon_loss_weight: 1.0    # Weight for reconstruction MSE loss

# Tokenizer configuration
tokenizer:
  type: "bpe"               # 'simple' for character-level, 'bpe' for byte pair encoding
  vocab_size: 10000         # Vocabulary size
  # BPE-specific settings:
  bpe_vocab_size: 10000     # Target vocab size for training

# Data configuration
data:
  type: "huggingface"             # 'dummy', 'huggingface', or 'text'
  batch_size: 32
  max_length: 256
  num_workers: 0
  num_examples: 1000        # For dummy dataset
  # For huggingface:
  dataset_name: "wikitext"
  dataset_config: "wikitext-2-raw-v1"

# Training configuration
training:
  learning_rate: 2e-5
  warmup_steps: 500
  weight_decay: 0.01
  ema_decay: 0.9999
  use_ema: true
  gradient_clip: 1.0
  num_epochs: 10
  max_steps: null           # Set to limit training steps (null = no limit)
  log_interval: 100
  val_interval: 500
  # CDLM Consistency Training (Together AI paper)
  use_consistency_training: false  # Enable for up to 14x faster inference
  consistency_loss_weight: 0.5    # Lambda weight for consistency loss (0.5-1.0)
  consistency_delta_min: 50       # Minimum timestep gap for consistency pairs
  consistency_delta_max: 200      # Maximum timestep gap for consistency pairs

# GPU/Device configuration
device:
  use_gpu: true             # Set to false to force CPU training
  multi_gpu: false          # Use DataParallel for multi-GPU
  mixed_precision: false    # Use automatic mixed precision (amp)
  benchmark: true           # Enable cudnn benchmark

# Inference configuration
inference:
  num_steps: 50             # Number of denoising steps (can be < num_diffusion_steps)
  temperature: 1.0
  top_k: null               # null to disable
  top_p: 0.95
  use_ddim: false           # Use DDIM sampling
  ddim_eta: 0.0

# Checkpoint configuration
checkpoint:
  save_dir: "./checkpoints"
  save_interval: 500        # Save every N steps
  keep_last_k: 3            # Keep last K checkpoints
