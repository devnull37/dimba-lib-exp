# DIMBA Model Configuration

# Model architecture parameters
model:
  d_model: 512              # Hidden dimension
  d_prompt: 512             # Prompt conditioning dimension
  num_diffusion_steps: 1000 # Number of diffusion steps T
  num_denoiser_layers: 6    # Number of Mamba-2 blocks
  d_state: 16               # SSM state size
  d_conv: 4                 # Convolution kernel size
  expand: 2                 # Expansion factor for Mamba
  conditioning_type: "film" # 'film' or 'additive'
  dropout: 0.1
  use_weight_tying: false   # Tie embedding and output weights

# Data configuration
data:
  type: "dummy"             # 'dummy', 'huggingface', or 'text'
  batch_size: 32
  max_length: 256
  num_workers: 0
  num_examples: 1000        # For dummy dataset
  # For huggingface:
  # dataset_name: "wikitext"
  # dataset_config: "wikitext-2-raw-v1"

# Training configuration
training:
  learning_rate: 2e-5
  warmup_steps: 500
  weight_decay: 0.01
  ema_decay: 0.9999
  use_ema: true
  gradient_clip: 1.0

# Inference configuration
inference:
  num_steps: 50             # Number of denoising steps (can be < num_diffusion_steps)
  temperature: 1.0
  top_k: null               # null to disable
  top_p: 0.95
  use_ddim: false           # Use DDIM sampling
  ddim_eta: 0.0
